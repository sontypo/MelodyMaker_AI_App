{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f663c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from shutil import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.fftpack import dct, idct\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f181c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- MDCT/IMDCT (approximate, using DCT-IV) ---------\n",
    "def mdct(x, n_fft):\n",
    "    # Pad to multiple of n_fft\n",
    "    pad = (n_fft - len(x) % n_fft) % n_fft\n",
    "    x = np.pad(x, (0, pad), mode='constant')\n",
    "    frames = librosa.util.frame(x, frame_length=n_fft, hop_length=n_fft//2).T\n",
    "    # DCT-IV\n",
    "    return dct(frames, type=2, axis=1, norm='ortho')\n",
    "\n",
    "def imdct(X, n_fft):\n",
    "    # Inverse DCT-IV\n",
    "    frames = idct(X, type=2, axis=1, norm='ortho')\n",
    "    # Overlap-add\n",
    "    hop = n_fft // 2\n",
    "    out = np.zeros((frames.shape[0] * hop + hop,))\n",
    "    for i, frame in enumerate(frames):\n",
    "        out[i*hop:i*hop+n_fft] += frame\n",
    "    return out\n",
    "\n",
    "# Add this class\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        q = self.query(x).reshape(B, C, -1)\n",
    "        k = self.key(x).reshape(B, C, -1)\n",
    "        v = self.value(x).reshape(B, C, -1)\n",
    "        attn = self.softmax(torch.bmm(q.transpose(1,2), k) / (C ** 0.5))\n",
    "        out = torch.bmm(attn, v.transpose(1,2)).transpose(1,2)\n",
    "        out = out.reshape(B, C, H, W)\n",
    "        return out + x\n",
    "\n",
    "# --------- Dataset with slicing and normalization ---------\n",
    "class AudioMDCTDataset(Dataset):\n",
    "    def __init__(self, file_list, rate=10000, feats=256, duration=3.3, total_seconds=26, hop_size=1, normalize=True):\n",
    "        self.rate = rate\n",
    "        self.feats = feats\n",
    "        self.duration = duration\n",
    "        self.segments = []\n",
    "        for file in file_list:\n",
    "            for offset in range(0, total_seconds, hop_size):\n",
    "                self.segments.append((file, offset))\n",
    "        self.normalize = normalize\n",
    "        self.mean = 0.0\n",
    "        self.std = 1.0\n",
    "        if normalize:\n",
    "            self._compute_stats()\n",
    "\n",
    "    def _compute_stats(self):\n",
    "        specs = []\n",
    "        for file, offset in self.segments[:min(100, len(self.segments))]:  # sample for stats\n",
    "            audio, sr = librosa.load(file, sr=self.rate, offset=offset, duration=self.duration)\n",
    "            audio_fill = np.zeros(int(self.rate * self.duration), dtype=np.float32)\n",
    "            audio_fill[:len(audio)] = audio\n",
    "            spec = mdct(audio_fill, self.feats)\n",
    "            specs.append(spec)\n",
    "        specs = np.stack(specs)\n",
    "        self.mean = specs.mean()\n",
    "        self.std = specs.std() + 1e-8\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, offset = self.segments[idx]\n",
    "        audio, sr = librosa.load(file, sr=self.rate, offset=offset, duration=self.duration)\n",
    "        audio_fill = np.zeros(int(self.rate * self.duration), dtype=np.float32)\n",
    "        audio_fill[:len(audio)] = audio\n",
    "        spec = mdct(audio_fill, self.feats)\n",
    "        if self.normalize:\n",
    "            spec = (spec - self.mean) / self.std\n",
    "        spec = np.expand_dims(spec, axis=0)  # (1, feats, time)\n",
    "        return torch.tensor(spec, dtype=torch.float32)\n",
    "\n",
    "# --------- Sinusoidal Embedding ---------\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dims=32, min_freq=1.0, max_freq=1000.0):\n",
    "        super().__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        frequencies = torch.exp(\n",
    "            torch.linspace(np.log(self.min_freq), np.log(self.max_freq), self.embedding_dims // 2, device=device)\n",
    "        )\n",
    "        angular_speeds = 2.0 * np.pi * frequencies\n",
    "        x = x.unsqueeze(-1)\n",
    "        embeddings = torch.cat([torch.sin(angular_speeds * x), torch.cos(angular_speeds * x)], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# --------- U-Net Blocks ---------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if not self.same_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x if self.same_channels else self.res_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.silu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x + residual\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block_depth):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            ResidualBlock(in_channels if i == 0 else out_channels, out_channels)\n",
    "            for i in range(block_depth)\n",
    "        ])\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x, skips):\n",
    "        x = self.blocks(x)\n",
    "        skips.append(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block_depth):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            ResidualBlock(in_channels if i == 0 else out_channels, out_channels)\n",
    "            for i in range(block_depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, skips):\n",
    "        x = self.upsample(x)\n",
    "        skip = skips.pop()\n",
    "        # Crop skip to match x's spatial size\n",
    "        if skip.shape[2:] != x.shape[2:]:\n",
    "            min_h = min(skip.shape[2], x.shape[2])\n",
    "            min_w = min(skip.shape[3], x.shape[3])\n",
    "            skip = skip[:, :, :min_h, :min_w]\n",
    "            x = x[:, :, :min_h, :min_w]\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, widths, block_depth, in_channels=1, embedding_dims=32, dim1=256, dim2=128):\n",
    "        super().__init__()\n",
    "        self.embedding = SinusoidalEmbedding(embedding_dims)\n",
    "        self.input_conv = nn.Conv2d(in_channels + embedding_dims, widths[0], kernel_size=1)\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(widths) - 1):\n",
    "            self.downs.append(DownBlock(widths[i], widths[i+1], block_depth))\n",
    "        self.mid = nn.Sequential(*[ResidualBlock(widths[-1], widths[-1]) for _ in range(block_depth)])\n",
    "        self.attention = SelfAttention2d(widths[-1])\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(widths) - 2, -1, -1):\n",
    "            self.ups.append(UpBlock(widths[i+1]*2, widths[i], block_depth))\n",
    "        self.output_conv = nn.Conv2d(widths[0], in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, noise_var):\n",
    "        batch, _, h, w = x.shape\n",
    "        # Ensure noise_var is (batch,)\n",
    "        if noise_var.dim() > 1:\n",
    "            noise_var = noise_var.view(batch)\n",
    "        e = self.embedding(noise_var.to(x.device))\n",
    "        e = e.unsqueeze(-1).unsqueeze(-1)  # (batch, embedding_dims, 1, 1)\n",
    "        e = e.expand(-1, -1, h, w)\n",
    "        x = torch.cat([x, e], dim=1)\n",
    "        x = self.input_conv(x)\n",
    "        skips = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, skips)\n",
    "        x = self.mid(x)\n",
    "        x = self.attention(x)\n",
    "        for up in self.ups:\n",
    "            x = up(x, skips)\n",
    "        x = self.output_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbef3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Losses ---------\n",
    "def spectral_norm_loss(pred, real):\n",
    "    norm_real = torch.norm(real, dim=(2,3)) + 1e-6\n",
    "    norm_pred = torch.norm(pred, dim=(2,3)) + 1e-6\n",
    "    return torch.mean(torch.abs(norm_real - norm_pred) / norm_real)\n",
    "\n",
    "def time_derivative_loss(pred, real, window=1):\n",
    "    real_deriv = real[:, :, :-window, :] - real[:, :, window:, :]\n",
    "    pred_deriv = pred[:, :, :-window, :] - pred[:, :, window:, :]\n",
    "    return F.mse_loss(real_deriv, pred_deriv)\n",
    "\n",
    "# --------- Diffusion Schedule ---------\n",
    "def diffusion_schedule(diffusion_times, min_signal_rate=0.02, max_signal_rate=0.95):\n",
    "    start_angle = torch.acos(torch.tensor(max_signal_rate))\n",
    "    end_angle = torch.acos(torch.tensor(min_signal_rate))\n",
    "    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "    signal_rates = torch.cos(diffusion_angles)\n",
    "    noise_rates = torch.sin(diffusion_angles)\n",
    "    return noise_rates, signal_rates\n",
    "\n",
    "# --------- Training Loop ---------\n",
    "def train(model, dataloader, optimizer, device, ema_model=None, ema_decay=0.999, epochs=1, mean=0.0, std=1.0, spec_mod=0.0, dx_mod=0.0):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            noises = torch.randn_like(batch)\n",
    "            batch_size = batch.size(0)\n",
    "            diffusion_times = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "            noise_rates, signal_rates = diffusion_schedule(diffusion_times)\n",
    "            noisy = signal_rates * batch + noise_rates * noises\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # pred_noises = model(noisy, noise_rates.squeeze(-1).squeeze(-1))\n",
    "            pred_noises = model(noisy, noise_rates.flatten())\n",
    "            # Crop pred and noises to the same shape\n",
    "            min_h = min(pred_noises.shape[2], noises.shape[2])\n",
    "            min_w = min(pred_noises.shape[3], noises.shape[3])\n",
    "            pred_noises = pred_noises[:, :, :min_h, :min_w]\n",
    "            noises = noises[:, :, :min_h, :min_w]\n",
    "            batch = batch[:, :, :min_h, :min_w]\n",
    "\n",
    "            loss = loss_fn(pred_noises, noises)\n",
    "            if spec_mod > 0:\n",
    "                loss += spec_mod * spectral_norm_loss(pred_noises, noises)\n",
    "            if dx_mod > 0:\n",
    "                loss += dx_mod * time_derivative_loss(pred_noises, noises)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                if ema_model is not None:\n",
    "                    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "                        ema_param.data.mul_(ema_decay).add_(param.data, alpha=1 - ema_decay)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# --------- Reverse Diffusion (DDIM) ---------\n",
    "def reverse_diffusion(model, initial_noise, diffusion_steps, device):\n",
    "    model.eval()\n",
    "    num_examples = initial_noise.shape[0]\n",
    "    step_size = 1.0 / diffusion_steps\n",
    "    next_noisy_data = initial_noise\n",
    "    for step in tqdm(range(diffusion_steps)):\n",
    "        noisy_data = next_noisy_data\n",
    "        diffusion_times = torch.ones((num_examples, 1), device=device) - step * step_size\n",
    "        noise_rates, signal_rates = diffusion_schedule(diffusion_times)\n",
    "        pred_noises = model(noisy_data, noise_rates)\n",
    "        # --- Crop all tensors to the minimum shape ---\n",
    "        min_h = min(noisy_data.shape[2], pred_noises.shape[2])\n",
    "        min_w = min(noisy_data.shape[3], pred_noises.shape[3])\n",
    "        noisy_data = noisy_data[:, :, :min_h, :min_w]\n",
    "        pred_noises = pred_noises[:, :, :min_h, :min_w]\n",
    "        noise_rates_ = noise_rates.view(-1,1,1,1).expand(-1,1,min_h,min_w)\n",
    "        signal_rates_ = signal_rates.view(-1,1,1,1).expand(-1,1,min_h,min_w)\n",
    "        pred_data = (noisy_data - noise_rates_ * pred_noises) / signal_rates_\n",
    "        next_diffusion_times = diffusion_times - step_size\n",
    "        next_noise_rates, next_signal_rates = diffusion_schedule(next_diffusion_times)\n",
    "        next_noise_rates_ = next_noise_rates.view(-1,1,1,1).expand(-1,1,min_h,min_w)\n",
    "        next_signal_rates_ = next_signal_rates.view(-1,1,1,1).expand(-1,1,min_h,min_w)\n",
    "        next_noisy_data = next_signal_rates_ * pred_data + next_noise_rates_ * pred_noises\n",
    "    return pred_data\n",
    "\n",
    "# --------- Generation ---------\n",
    "def generate(model, shape, device, diffusion_steps=1000, mean=0.0, std=1.0):\n",
    "    with torch.no_grad():\n",
    "        initial_noise = torch.randn(shape).to(device)\n",
    "        generated_data = reverse_diffusion(model, initial_noise, diffusion_steps, device)\n",
    "        # Denormalize\n",
    "        generated_data = generated_data * std + mean\n",
    "        return generated_data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e905661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training and generating for genre: blues\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [29:10<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1469702124595642\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [29:07<00:00, 10.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.08760330080986023\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [29:03<00:00, 10.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11753316223621368\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [29:03<00:00, 10.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14250564575195312\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:48<00:00, 10.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.09125635027885437\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:48<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.07614340633153915\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:53<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15747524797916412\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:55<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11004752665758133\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:55<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11295919120311737\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:59<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11356737464666367\n",
      "Generating 10 samples for genre: blues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:40<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and generating for genre: classical\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [29:08<00:00, 10.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.16803498566150665\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [28:00<00:00, 10.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2830210030078888\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:51<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.20761965215206146\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:54<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13871783018112183\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:52<00:00, 10.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15734903514385223\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:51<00:00, 10.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.10454849898815155\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:50<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1480257511138916\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:51<00:00, 10.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15202981233596802\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:50<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.12792852520942688\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:53<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11239919066429138\n",
      "Generating 10 samples for genre: classical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and generating for genre: country\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:51<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15719641745090485\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [27:53<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1147167980670929\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/163 [00:20<27:59, 10.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# You can increase epochs as needed\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamples_per_genre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples for genre: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m samples \u001b[38;5;241m=\u001b[39m generate(ema_model, (samples_per_genre, \u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m2\u001b[39m], shape[\u001b[38;5;241m3\u001b[39m]), device, diffusion_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device, ema_model, ema_decay, epochs, mean, std, spec_mod, dx_mod)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m---> 27\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         noises \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(batch)\n\u001b[0;32m     29\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "genres = [\n",
    "    \"blues\", \"classical\", \"country\", \"disco\", \"hiphop\",\n",
    "    \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"\n",
    "]\n",
    "\n",
    "samples_per_genre = 10\n",
    "results = {}\n",
    "\n",
    "for genre in genres:\n",
    "    print(f\"Training and generating for genre: {genre}\")\n",
    "    music_files = glob.glob(f\"../genres/{genre}/*.au\")\n",
    "    if len(music_files) == 0:\n",
    "        continue\n",
    "\n",
    "    dataset = AudioMDCTDataset(\n",
    "        music_files, rate=10000, feats=256, duration=30.0, total_seconds=30, hop_size=1, normalize=True\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Get shape and normalization stats\n",
    "    for test_batch in dataloader:\n",
    "        shape = test_batch.shape\n",
    "        break\n",
    "    mean = dataset.mean\n",
    "    std = dataset.std\n",
    "\n",
    "    model = UNet(widths=[128, 128, 128, 128], block_depth=2,\n",
    "                    in_channels=1, dim1=shape[2], dim2=shape[3]).to(device)\n",
    "    ema_model = copy.deepcopy(model)\n",
    "    ema_decay = 0.999\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    num_total_examples = len(dataset) // shape[0]\n",
    "    for epoch in range(10):  # You can increase epochs as needed\n",
    "        print(f\"Epoch {epoch+1}/10\")\n",
    "        train(model, dataloader, optimizer, device, ema_model=ema_model, ema_decay=ema_decay, epochs=1, mean=mean, std=std, spec_mod=0.1, dx_mod=0.1)\n",
    "\n",
    "    print(f\"Generating {samples_per_genre} samples for genre: {genre}\")\n",
    "    samples = generate(ema_model, (samples_per_genre, 1, shape[2], shape[3]), device, diffusion_steps=1000, mean=mean, std=std)\n",
    "    results[genre] = samples\n",
    "\n",
    "    # --- Save generated samples as audio files ---\n",
    "    os.makedirs(f\"generated/{genre}\", exist_ok=True)\n",
    "    for i, spec in enumerate(samples):\n",
    "        spec_np = spec.squeeze().cpu().numpy()\n",
    "        audio = imdct(spec_np, 256)\n",
    "        audio = audio[:int(30.0 * 10000)]  # Ensure 30 seconds\n",
    "        sf.write(f\"generated/{genre}/sample_{i+1}.wav\", audio, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
